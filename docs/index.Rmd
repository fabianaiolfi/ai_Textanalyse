---
title: "Textanalyse"
author: "Fabian Aiolfi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(here)
library(tidyverse)
library(tidytext)
library(tm)

setwd(here())
```

```{r Data Import}
ai_full <- read.csv("../data/swissdox/read_csv_example/jk_export.csv")
```

```{r Examination and Duplicates}
# nrow(ai_full) # 6597 rows
# ai_full %>% select(id) %>% distinct(id) # 6597
# ai_full %>% select(ht) %>% distinct(ht) # 5917
# 
# unique(ai_full$ct)
# hist(ai_full$nz)
```

```{r Clean Data}
unknown_sections <- c("", " ", "dl", "hv", "if", "ip", "iu", "iw", "kc")
boerse <- c("Börse Schweiz Text","Börsen und Märkte","Börsen und Märkte (bm)","Börse Schweiz Text (WIRTSCHAFT)","Börsen und Märkte","Börsen und Märkte (bm)")
feuilleton <- c("feuilleton","Feuilleton","Feuilleton (fe)")
finanzen <- c("finanzen","Finanzen (bm)")
fokus_w <- c("Fokus der Wirtschaft","Fokus der Wirtschaft (fw)")
forschung <- c("Forschung und Technik (fo)","Forschung und Technik (ft)")
front <- c("Front","Front (fp)","Startseite")
meinung <- c("meinung","Meinung","Meinung und Debatte","Meinung und Debatte (oe)", "Kommentare")
reflexe <- c("Reflexe","Reflexe (rx)","Reflexe Seiten")
schweiz <- c("schweiz","Schweiz","Schweiz (il)","schweiz")
vermischt <- c("Vermischte Meldungen","Vermischtes (vm)","Wetter/Vermischtes","Vermischtes")
wirtschaft <- c("Themen/Thesen Wirtschaft","Wirtschaft Text (WIRTSCHAFT) BLICKPUNKT BERGIER-BERICHTE","wirtschaft","Wirtschaft Text")

ai_clean <- ai_full %>% select(id, so_txt, pubDate, rq, ru, ht, sm, ut, nz, ct, tx) %>% 
  filter(ct == 1) %>% 
  select(!ct) %>% 
  distinct(ht, .keep_all = T) %>% 
  mutate(pubDate = as.Date(pubDate, format =  "%Y-%m-%d")) %>% 
  # Clean Section
  mutate(rubrik = paste(rq, ru, sep = " ")) %>%
  mutate(rubrik = str_replace(rubrik, "^\\ ", "")) %>%  # remove space at beginning
  mutate(rubrik = str_replace(rubrik, "\\ $", "")) %>% # remove space at end
  mutate(rubrik = str_replace(rubrik, "\\ \\(\\S+\\)$", "")) %>% # remove brackets
  mutate(rubrik = case_when(rubrik %in% unknown_sections ~ "Unbekannt",
                            rubrik == "wd" ~ "WD",
                            rubrik == "Basel (ni)" ~ "Basel",
                            rubrik == "Bau- und Immobilienmarkt (qw)" ~ "Bau- und Immobilienmarkt",
                            rubrik %in% boerse ~ "Börse und Märkte",
                            rubrik == "Branchenfokus (fk)" ~ "Branchenfokus",
                            rubrik == "Briefe an die NZZ (br)" ~ "Briefe an die NZZ",
                            rubrik == "Dossier Medien (dm)" ~ "Dossier Medien",
                            rubrik %in% feuilleton ~ "Feuilleton",
                            rubrik %in% finanzen ~ "Finanzen",
                            rubrik %in% fokus_w ~ "Fokus der Wirtschaft",
                            rubrik %in% forschung ~ "Forschung und Technik",
                            rubrik %in% front ~ "Front",
                            rubrik == "Geld und Anlage (ga)" ~ "Geld und Anlage",
                            rubrik %in% meinung ~ "Meinung",
                            rubrik %in% reflexe ~ "Reflexe",
                            rubrik %in% vermischt ~ "Vermischte Meldungen",
                            rubrik == "Zürich" ~ "Zürich und Region",
                            rubrik == "Inland Text" ~ "Inland",
                            rubrik == "Schwerpunkt Schwerpunkt" ~ "Schwerpunkt",
                            rubrik %in% wirtschaft ~ "Wirtschaft",
                            rubrik == "Schwerpunkt Schwerpunkt" ~ "Schwerpunkt",
                            rubrik == "Schwerpunkt Schwerpunkt" ~ "Schwerpunkt",
                            rubrik == "Ausland Text" ~ "Ausland",
                            rubrik == "Börsen und Märkte" ~ "Börse und Märkte",
                            rubrik == "Front Page" ~ "Front",
                            rubrik == "OP-ED Seite" ~ "Meinung",
                            rubrik %in% schweiz ~ "Schweiz",
                            TRUE ~ as.character(rubrik)))

# Clean tx
# ai_clean$tx[round(runif(1, min=0, max=5889), 0)] # Display random articles
# ai_clean$tx_clean[round(runif(1, min=0, max=5889), 0)])

ai_clean <- ai_clean %>% 
  mutate(tx_clean = str_replace_all(tx, "<[^>]+>", "")) %>%  # remove all < > brackets incl. text
  mutate(tx_clean_nchar = nchar(tx_clean))
```

# ai. Textanalyse

## Was das ist

-   Textanalyse der Artikelsammlung von Sergio Aiolfi
-   Artikel aus SwissDox-Datenbank, die mit den Suchbegriffen «Sergio Aiolfi» oder «ai.» aufgetaucht sind

## Umfang der Analyse

```{r Umfang}
# Number of articles
number_of_articles <- nrow(ai_clean)

# Total number of words
ai_clean <- ai_clean %>% mutate(word_count = str_count(tx_clean, '\\w+'))
total_wc <- sum(ai_clean$word_count) # 2'782'881

# Average word length per article
ai_clean <- ai_clean %>% mutate(avg_word_len = ((tx_clean_nchar - word_count) / word_count))
plot(ai_clean$word_count, ai_clean$avg_word_len)
  
# First article
first_date <- min(ai_clean$pubDate)
first_date_title <- ai_clean %>% select(pubDate, ht) %>% 
  filter(pubDate == first_date)
first_date_title <- first_date_title[,2]

# Last article
last_date <- max(ai_clean$pubDate)
last_date_title <- ai_clean %>% select(pubDate, ht) %>% 
  filter(pubDate == last_date)
last_date_title <- last_date_title[1,2]

# Date Range
time_range <- difftime(last_date, first_date, units="days")/365
time_range <- as.numeric(time_range)
time_range <- round(time_range, 1)

# Nicely format date string
first_date <- format(first_date, "%d %B %Y")
last_date <- format(last_date, "%d %B %Y")

# Total number of characters
total_nchar <- sum(ai_clean$tx_clean_nchar) # 20'778'399

# Unique Words
#unique_words <- nrow(ai_corpus_unique_words) # 112'925 # this will throw an error becaus ai_corpus_unique_words is defined further below
```

-   number_of_articles Artikel werden untersucht über einer Zeitspanne von time_range Jahren
- Anzahl Wörter: total_wc
- Anzahl Zeichen: total_nchar
- Wortschatzgrösse: unique_words
-   Erster Artikel in den Daten: first_date_title, publiziert am first_date
-   Neuster Artikel in den Daten: last_date_title, publiziert am last_date

## Ungenauigkeiten

-   Duplikate Artikel, z.B. gedruckte Zeitung und nzz.ch
-   Aritkel anderer Journis, z.B. Beat Gygi

## Analyse der Rubriken

```{r Sections Count}
# Count all categories
rubrik_count <- ai_clean %>%
  select(rubrik) %>% 
  count(rubrik)

ggplot(rubrik_count, aes(y = reorder(rubrik, n), x = n)) +
  geom_bar(stat="identity") +
  geom_text(aes(label = format(round(n, digits = 0))), vjust=0.5, hjust=-0.25) +
  theme_minimal()
```

```{r Sections over Time}
# Convert low category count to "Andere"
rubrik_other <- rubrik_count %>% 
  filter(n < 145) %>% 
  select(rubrik)

rubrik_other <- as.array(rubrik_other$rubrik)

ai_clean <- ai_clean %>% 
  mutate(rubrik_graph = case_when(rubrik %in% rubrik_other ~ "Andere",
                                  TRUE ~ as.character(rubrik)))

section_over_time <- ai_clean %>% select(pubDate, rubrik_graph) %>% 
  mutate(year = format(pubDate, format="%Y")) %>%
  count(year, rubrik_graph)

ggplot(section_over_time, aes(x=year, y=n, fill=rubrik_graph)) +
  geom_bar(stat="identity") +
  theme_minimal()

# To do
# Andere and Unbekannt together
# evtl. 5-7 categories
```

## Analyse über Zeit
```{r Number of articles per year over time}
articles_by_year <- ai_clean %>% select(id, pubDate) %>% 
  mutate(year = format(pubDate, format="%Y")) %>% 
  count(year)

ggplot(articles_by_year, aes(x=year, y=n)) +
  geom_bar(stat="identity")
```

```{r Number of articles over all time by month}
articles_by_month <- ai_clean %>% select(id, pubDate) %>%
  mutate(month = format(pubDate, format="%m")) %>%
  count(month)

ggplot(articles_by_month, aes(x=month, y=n)) +
  geom_bar(stat="identity")
```

```{r Print vs online over time}
pub_over_time <- ai_clean %>%
  select(pubDate, so_txt) %>% 
  mutate(year = format(pubDate, format="%Y")) %>% 
  count(so_txt, year)

ggplot(pub_over_time, aes(x=year, y=n, fill=so_txt)) +
  geom_bar(stat="identity")
```


## Artikellänge
```{r Article length over time}
length_over_time <- ai_clean %>% select(pubDate, nz) %>% 
  mutate(year = format(pubDate, format="%Y")) %>% 
  group_by(year) %>% 
  summarise(nz_mean = mean(nz, na.rm=TRUE))

ggplot(length_over_time, aes(x=year, y=nz_mean)) +
  geom_bar(stat="identity")
# maybe add min and max of each year too, as a spread?
```

```{r Article character length (with spaces) over time: Spread Boxplot}
len_over_time <- ai_clean %>% 
  select(pubDate, tx_clean_nchar) %>% 
  mutate(year = format(pubDate, format="%Y"))

ggplot(len_over_time, aes(x = year, y = tx_clean_nchar, group = year)) +
  geom_boxplot(coef = 10)
```

### Kürzester und längster Artikel
```{r Shortest article}
shortest_article <- ai_clean %>%
  select(pubDate, so_txt, rubrik, ht, sm, ut, tx_clean, tx_clean_nchar) %>% 
  slice(which.min(tx_clean_nchar))

shortest_article_title <- shortest_article$ht
shortest_article_date <- shortest_article$pubDate
shortest_article_rubrik <- shortest_article$rubrik
shortest_article_len <- shortest_article$tx_clean_nchar
```
- Kürzester Artikel: shortest_article_title (shortest_article_rubrik), publiziert am shortest_article_date mit shortest_article_len Zeichen

```{r Longest article}
longest_article <- ai_clean %>%
  select(pubDate, so_txt, rubrik, ht, sm, ut, tx_clean, tx_clean_nchar) %>% 
  slice(which.max(tx_clean_nchar))

longest_article_title <- longest_article$ht
longest_article_date <- longest_article$pubDate
longest_article_rubrik <- longest_article$rubrik
longest_article_len <- longest_article$tx_clean_nchar
```
- Längster Artikel: longest_article_title (longest_article_rubrik) am longest_article_date mit einer Länge von longest_article_len

### Titellängen
```{r Longest Title}
ai_clean <- ai_clean %>% mutate(title_len = str_count(ht, '\\w+'))

# shortest_title <- ai_clean %>%
#   select(pubDate, so_txt, rubrik, ht, sm, ut, tx_clean, title_len) %>% 
#   slice(which.min(title_len))
# not very interesting!

longest_title <- ai_clean %>%
  select(pubDate, so_txt, rubrik, ht, sm, ut, tx_clean, title_len) %>%
  slice(which.max(title_len))

longest_title_title <- longest_title$ht
longest_title_date <- longest_title$pubDate
longest_title_rubrik <- longest_title$rubrik
```

```{r Average Title Length over Time}
title_len_over_time <- ai_clean %>% 
  select(pubDate, title_len) %>% 
  mutate(year = format(pubDate, format="%Y"))

ggplot(title_len_over_time, aes(x = year, y = title_len, group = year)) +
  geom_boxplot(coef = 10)
```

## Textanalyse: Ngrams

```{r Ngram: Data Preperation, warning=FALSE, cache=TRUE}
# Convert the text column to a corpus
ai_corpus <- Corpus(VectorSource(ai_clean$tx_clean))

# Custom stop words
custom_stop_words <- c("amp", "x00a0", "sergio", "aiolfi", "ai", "dass", "i", "v", "u", "a", "der", "die", "das", "in", "worden", "wurde", "xa")
error_words <- c("amp", "x00a0", "i", "v", "u", "a", "xa")

# Remove stopwords using the tm library's tm_map function
ai_corpus_no_stopwords <- tm_map(ai_corpus, removeWords, stopwords("german"))
ai_corpus_no_stopwords <- tm_map(ai_corpus_no_stopwords, content_transformer(tolower))
ai_corpus_no_stopwords <- tm_map(ai_corpus_no_stopwords, removeNumbers)
ai_corpus_no_stopwords <- tm_map(ai_corpus_no_stopwords, removeWords, custom_stop_words)
ai_corpus_no_stopwords <- data.frame(text = sapply(ai_corpus_no_stopwords, as.character), stringsAsFactors = FALSE) # convert corpus to DF

# Keep a corpus with stopwords
ai_corpus_with_stopwords <- tm_map(ai_corpus, content_transformer(tolower))
ai_corpus_with_stopwords <- tm_map(ai_corpus_with_stopwords, removeNumbers)
ai_corpus_with_stopwords <- tm_map(ai_corpus_with_stopwords, removeWords, error_words)
ai_corpus_with_stopwords <- data.frame(text = sapply(ai_corpus_with_stopwords, as.character), stringsAsFactors = FALSE) # convert corpus to DF
```

### 10 häufigsten Wörter (ohne Stopwörter)
- Stopwörter entfernt
- Satzzeichen entfernt
- Alles kleingeschrieben
```{r Unigram Graph, cache=TRUE}
ai_corpus_unigram <- ai_corpus_no_stopwords %>%
  unnest_tokens(unigram, text, token = "ngrams", n = 1) %>% # n = 2 for bigrams etc, n=1 for most popular word
  filter(!is.na(unigram)) %>% 
  count(unigram, sort = TRUE)

ai_corpus_unique_words <- ai_corpus_unigram %>% select(unigram)

# Get unusual words (count equals 1) for later on
ai_corpus_unusual <- ai_corpus_unigram %>% filter(n == 1) %>% select(unigram)

ai_corpus_unigram <- head(ai_corpus_unigram, n = 10)

ggplot(ai_corpus_unigram, aes(y = reorder(unigram, n), x = n)) +
  geom_bar(stat="identity") +
  geom_text(aes(label = format(round(n, digits = 0))), vjust=0.5, hjust=-0.25) +
  theme_minimal()
```

### Häufigste Wörter pro Jahr
```{r Unigram by Year}
rm(unigram_by_year)

unigram_by_year <- ai_clean %>% 
  select(pubDate) %>% 
  mutate(id = row_number())

ai_corpus_no_stopwords_id <- ai_corpus_no_stopwords %>% 
  mutate(id = row_number())

unigram_by_year <- unigram_by_year %>% 
  left_join(ai_corpus_no_stopwords_id, by = "id") %>% 
  select(!id) %>% 
  mutate(year = format(pubDate, format="%Y"))

unigram_by_year <- unigram_by_year %>%
  unnest_tokens(unigram, text, token = "ngrams", n = 1) %>%
  filter(!is.na(unigram)) %>% 
  count(unigram, year) %>%  #, sort = TRUE)
  group_by(year) %>% 
  top_n(3, n)
```


### 10 häufigste Wortpaare (ohne Stopwörter)
```{r Bigram Graph, cache=TRUE}
ai_corpus_bigram <- ai_corpus_no_stopwords %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% # n = 2 for bigrams etc, n=1 for most popular word
  filter(!is.na(bigram)) %>% 
  count(bigram, sort = TRUE)

ai_corpus_bigram <- head(ai_corpus_bigram, n = 10)

ggplot(ai_corpus_bigram, aes(y = reorder(bigram, n), x = n)) +
  geom_bar(stat="identity") +
  geom_text(aes(label = format(round(n, digits = 0))), vjust=0.5, hjust=-0.25) +
  theme_minimal()

# To do: Show bigrams in context?
```

### 10 häufigste Trigramme (inkl. Stopwörter)
```{r Trigram Graph, cache=TRUE}
ai_corpus_trigram <- ai_corpus_with_stopwords %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% # n = 2 for bigrams etc, n=1 for most popular word
  filter(!is.na(trigram)) %>% 
  count(trigram, sort = TRUE)

ai_corpus_trigram <- head(ai_corpus_trigram, n = 10)

ggplot(ai_corpus_trigram, aes(y = reorder(trigram, n), x = n)) +
  geom_bar(stat="identity") +
  geom_text(aes(label = format(round(n, digits = 0))), vjust=0.5, hjust=-0.25) +
  theme_minimal()

# To do: Show trigrams in context?
```

### 10 häufigste Viergramme (inkl. Stopwörter)
```{r Fourgram Graph, cache=TRUE}
ai_corpus_fourgram <- ai_corpus_with_stopwords %>%
  unnest_tokens(fourgram, text, token = "ngrams", n = 4) %>% # n = 2 for bigrams etc, n=1 for most popular word
  filter(!is.na(fourgram)) %>% 
  count(fourgram, sort = TRUE)

ai_corpus_fourgram <- head(ai_corpus_fourgram, n = 10)

ggplot(ai_corpus_fourgram, aes(y = reorder(fourgram, n), x = n)) +
  geom_bar(stat="identity") +
  geom_text(aes(label = format(round(n, digits = 0))), vjust=0.5, hjust=-0.25) +
  theme_minimal()

# To do: Show fourgrams in context?
```

## 10 ungewöhnliche Wörter
```{r Ngrams: Unusual Words}
# Ie words that occur only once in the corpus
sample(ai_corpus_unusual$unigram, 10)
ai_corpus_unusual_size <- nrow(ai_corpus_unusual)
```
- Ungewöhnlich = erscheinen nur 1 Mal
- Anzahl Wörter, die nur 1 Mal erscheinen: ai_corpus_unusual_size

## Textanalyse: Wortlänge
```{r Longest Word}
# List all unqiue words
# see Unigram Graph

# Count characters of unique words
ai_corpus_unique_words <- ai_corpus_unique_words %>% 
  mutate(char_len = nchar(unigram)) %>% 
  arrange(desc(char_len))
# lotta noise, slightly pointless

# Show top n long words

```


## Textanalse: Satzlänge
```{r Average Sentence Length of each article}
# Function to measure average sentence length
measure_avg_sentence_length <- function(text) {
  # Split the text into sentences
  sentences <- strsplit(text, "(?<=[.?!])\\s+(?=[A-Z])", perl = TRUE)[[1]]
  
  # Calculate the number of words in each sentence
  sentence_lengths <- sapply(sentences, function(x) length(strsplit(x, "\\s+")[[1]]))
  
  # Calculate the average sentence length
  avg_sentence_length <- mean(sentence_lengths)
  
  return(avg_sentence_length)
}

ai_clean <- ai_clean %>%
  mutate(avg_sent_len = map(tx_clean, measure_avg_sentence_length))
ai_clean$avg_sent_len <- as.numeric(ai_clean$avg_sent_len)
```

```{r Average Sentence Length over time}
sent_len_over_time <- ai_clean %>% 
  select(pubDate, avg_sent_len, rubrik) %>% 
  mutate(year = format(pubDate, format="%Y"))

sent_len_over_time$avg_sent_len <- as.numeric(sent_len_over_time$avg_sent_len)

ggplot(sent_len_over_time, aes(x = year, y = avg_sent_len, group = year)) +
  geom_boxplot(coef = 10)
```


```{r Average Sentence Length over time by Rubrik}
top_rubrik <- c("Ausland", "Wirtschaft", "Börse und Märkte", "Inland", "Reflexe", "Unternehmenspraxis")

sent_len_by_rubik <- sent_len_over_time %>% 
  group_by(year, rubrik) %>% 
  dplyr::summarize(len_mean = mean(avg_sent_len, na.rm=TRUE)) %>% 
  filter(rubrik %in% top_rubrik)

ggplot(sent_len_by_rubik, aes(x = year, y = len_mean, group = rubrik)) +
  geom_line(aes(color=rubrik))
```


## Textanalyse: Unwörter
```{r Articles with at least one Unwort, eval=FALSE, warning=FALSE, include=FALSE}
unwoerter <- c("beinhaltet", "beinhalten", "Dunkelziffer", "schlussendlich")

unwoerter_corpus <- ai_clean %>%
  select(pubDate, ht, rubrik, tx_clean) %>%
  mutate(unwort = str_detect(tx_clean, unwoerter))

summary(unwoerter_corpus$unwort)
```
- Talks the talk, walks the walk: Keiner der Artikel beinhalten Unwörter. «Dunkelziffer» erscheint nur 1 Mal, und zwar in einem Aritkel über den Begriff «Dunkelziffer».

## Textanalyse: Gendergerechte Sprache
```{r eval=FALSE, include=FALSE}
gender_words <- c("innen")

gender_corpus <- ai_clean %>%
  select(pubDate, ht, rubrik, tx_clean) %>%
  mutate(gender = str_detect(tx_clean, "innen ")) %>% 
  mutate(year = format(pubDate, format="%Y")) %>% 
  filter(gender == TRUE) %>% 
  count(year, gender)
  
ggplot(gender_corpus, aes(x = year, y = n)) +
  geom_bar(stat = "identity")

# not working as intended yet
```

## Textanalyse: Anglizismen
```{bash eval=FALSE, include=FALSE}
# Search and replace in anglizismen_02.csv
sed -i'' -e 's/"/ /g' anglizismen_02.csv
sed -i'' -e 's/ ,  /\n/g' anglizismen_02.csv
```

```{r}
anglizismen <- readLines("anglizismen_02.csv")
anglizismen <- tolower(anglizismen)
anglizismen <- paste(anglizismen, collapse = "|")

anglizismen_corpus <- ai_clean %>%
  select(pubDate, ht, rubrik, tx_clean)

anglizismen_corpus$tx_clean <- tolower(anglizismen_corpus$tx_clean)

anglizismen_corpus <- anglizismen_corpus %>%
  filter(!str_detect(tx_clean, anglizismen))
```
- Aus einer Liste von 4724 Anglizismen [https://de.wiktionary.org/wiki/Verzeichnis:Deutsch/Anglizismen], werden nur ganz wenige verwendet


## Archiv
```{r Archive, eval=FALSE, include=FALSE}
# Clean Sections
#length(unique(ai_clean$ru)) # 64

# sections_ru <- ai_clean %>% select(ru) %>% 
#   count(ru) %>% 
#   filter(n > 10)

# sections_rq <- ai_clean %>% select(rq) %>% 
#   count(rq)

# sections_na <- ai_clean %>% filter(ru == "" & rq == "")

# sections_both <- ai_clean %>% select(ru, rq) %>% count(ru, rq)

# ggplot(sections_ru, aes(x=ru, y=n)) +
#   geom_bar(stat="identity")
# 
# length(unique(ai_clean$rubrik)) # 122

# rubrik <- ai_clean %>%
#   select(rubrik) %>% 
#   distinct(rubrik)
```

